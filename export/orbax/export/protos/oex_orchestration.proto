syntax = "proto3";

package orbax_export_orchestration;

// Multiple orchestration pipelines.
//
// A pipeline consists of a model function, a (optional) pre-processor and
// a (optional) post-processor.
message Pipelines {
  // A map from pipeline name to pipeline.
  //
  // Pipeline names usually correspond to Servo service names, such as
  // "serving_default".
  map<string, Pipeline> name_to_pipeline = 1;
}

// An orchestration pipeline.
message Pipeline {

  // The names (in manifest) of the pre-processor functions. For multi-stage
  // preprocessing, the preprocessors are exected in sequence. The output of the
  // nth pre-processor serves as the input to the (n+1)th pre-processor. while
  // multi-stage tf preprocessing is doable, we recommend consolidating the
  // steps into a single module to optimize for latency.
  repeated string pre_processor_names = 20;

  // The model functions to be executed in the pipeline. The order of the
  // model functions determines the order of execution. The output of the nth
  // model function serves as the input to the (n+1)th model function.
  // Currently, we ONLY support AT MOST ONE model function (thus, having no
  // model functions is allowed). See b/421976850.
  repeated BoundModelFunction model_functions = 30;

  // The names (in manifest) of the post-processor functions. For multi-stage
  // postprocessing, the postprocessors are exected in sequence. The output of
  // the nth post-processor serves as the input to the (n+1)th post-processor.
  // while multi-stage tf postprocessing is doable, we recommend consolidating
  // the steps into a single module to optimize for latency.
  repeated string post_processor_names = 50;

  // The batch options for the model. If not set, the model will not be batched.
  optional BatchOptions batch_options = 60;
}

// A bound model function.
//
// This message is used to represent a model function that has been bound to
// a specific weights PyTree. This is needed because the model function will
// need to be called with the weights as the 1st argument.
message BoundModelFunction {
  // The name (in manifest) of the model function.
  string model_function_name = 1;

  // The name (in manifest) of the value representing the weights PyTree.
  // This PyTree will be given as the 1st arg to the model function.
  optional string weights_name = 10;
}

message BatchOptions {
  // The component of the model to batch.
  enum BatchComponent {
    BATCH_COMPONENT_UNSPECIFIED = 0;
    // No batching.
    NO_BATCHING = 1;
    // The model function corresponds to `model_function_name` in the
    // orchestration.
    MODEL_FUNCTION = 2;
    // This option batches all available components, which can be a
    // pre-processor, model function and post-processor. The pre-processor and
    // post-processor correspond to `pre_processor_name` and
    // `post_processor_name` in the orchestration.
    WHOLE_PIPELINE = 3;
    // This option includes two components: pre-processor and model function.
    PRE_PROCESSOR_AND_MODEL_FUNCTION = 4;
    // This option includes two components: model function and post-processor.
    MODEL_FUNCTION_AND_POST_PROCESSOR = 5;
  }

  // The component of the model to batch.
  BatchComponent batch_component = 1;

  // The maximum allowed batch size for any input.
  int32 max_batch_size = 2;

  // Maximum number of microseconds to wait before outputting an incomplete
  // batch.
  int32 batch_timeout_micros = 3;

  // Optional list of allowed batch sizes. If left empty, all batch sizes no
  // larger than `max_batch_size` are allowed. Otherwise, supplies a list of
  // batch sizes, causing the op to pad batches up to one of those sizes. The
  // entries must increase monotonically, and the final entry must equal
  // `max_batch_size`.
  repeated int32 allowed_batch_sizes = 4;

  // If false, an input task with a large size will be split into multiple
  // smaller batch tasks and possibly put into different batches for processing.
  // If true, each input task is put into one batch as a whole for processing.
  // More padding will be needed.
  bool disable_large_batch_splitting = 5;

  // Number of scheduling threads for processing batches of work. Determines
  // the number of batches processed in parallel. This should be roughly in line
  // with the number of TPU cores available.
  int32 num_batch_threads = 6;

  // Maximum number of batches enqueued for processing before requests are
  // failed fast.
  int32 max_enqueued_batches = 7;

  // An enum class containing possible values for the batch_padding_policy. This
  // argument specifies the policy that a batch scheduler is using when deciding
  // what to do when, say, 18 requests need to be batched, but only 16 and 32
  // batch sizes are allowed. The following options are available.
  //
  //   - PAD_UP: pad to size 32.
  //   - BATCH_DOWN: schedule a batch of size 16 and leave 2 requests in the
  //     batch buffer.
  //   - MINIMIZE_TPU_COST_PER_REQUEST: a smarter greedy policy that chooses
  //     to either PAD_UP or BATCH_DOWN so as to minimize the TPU costs per
  //     real request. In this case, it would compare (batch_16_cost / 16) and
  //     (batch_32_cost / 18).
  enum BatchPaddingPolicy {
    PADDING_POLICY_UNSPECIFIED = 0;
    // Pad up to the next allowed batch size.
    PAD_UP = 1;
    // TODO: b/416071759 - Add BATCH_DOWN and MINIMIZE_TPU_COST_PER_REQUEST
    // policies once they're supported in JSV.
  }

  // The padding policy for the batch.
  BatchPaddingPolicy batch_padding_policy = 8;
}
