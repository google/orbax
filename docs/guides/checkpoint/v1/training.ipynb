{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnKdma9TUAdz"
      },
      "source": [
        "# Checkpointing in a Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvkv-cCsfXfq"
      },
      "source": [
        "This guide covers the usage of the `training` module, designed around the basic\n",
        "concept of a training loop. \u003cbr\u003e\u003cbr\u003e Note: We use the `--xla_force_host_platform_device_count=8` flag to emulate multiple devices in our single-CPU environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9rJjIWVfXfq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8'\n",
        "\n",
        "from orbax.checkpoint import v1 as ocp\n",
        "from etils import epath\n",
        "\n",
        "training = ocp.training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "friVOPN9fXfq"
      },
      "source": [
        "## Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZLx-9EVfXfq"
      },
      "source": [
        "Let's dive in with a simple training loop example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7OeZ_yqmnAn"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Dlz9nM6fXfq"
      },
      "source": [
        "We will use the `Checkpointer` API provided by the `training` module. The\n",
        "`Checkpointer` must be configured with a **root directory**, which represents a\n",
        "working directory where all checkpoints will be saved throughout the course of\n",
        "an experiment.\n",
        "\n",
        "The root directory is not itself a checkpoint; rather, it is a *container* of\n",
        "checkpoints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ol426K07fXfq"
      },
      "outputs": [],
      "source": [
        "root_directory = epath.Path('/tmp/my-checkpoints-1')\n",
        "root_directory.rmtree(missing_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yKDLOOHfXfq"
      },
      "source": [
        "We will assume the existence of a training state containing the keys `params`\n",
        "and `opt_state`, which are trees of `jax.Array`. The state also contains a key\n",
        "`step`, which is represented as an integer.\n",
        "\n",
        "Note that the arrays in the state will be sharded using a fully-replicated\n",
        "sharding, but the example would work equally well with any other sharding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYsVCumVfXfq"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import numpy as np\n",
        "\n",
        "pytree = {\n",
        "    'params': {\n",
        "        'layer0': np.arange(16).reshape((8, 2)),\n",
        "    },\n",
        "    'opt_state': [np.arange(16)],\n",
        "}\n",
        "sharding = jax.sharding.NamedSharding(\n",
        "    jax.sharding.Mesh(jax.devices(), ('x',)), jax.sharding.PartitionSpec()\n",
        ")\n",
        "pytree = jax.tree.map(lambda x: jax.device_put(x, sharding), pytree)\n",
        "pytree['step'] = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6X0fL2jxfXfq"
      },
      "source": [
        "Let's set up our fake training loop. We will add a \"training step function\" that\n",
        "just increments the step. In reality, this would also compute gradients and\n",
        "update model parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHCqG_6WfXfq"
      },
      "outputs": [],
      "source": [
        "def train_step(state):\n",
        "  state['step'] += 1\n",
        "  return state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYWpb0qNfXfq"
      },
      "source": [
        "Now, we can create a `Checkpointer` to begin saving a sequence of checkpoints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p926tCf8fXfq"
      },
      "outputs": [],
      "source": [
        "with training.Checkpointer(root_directory) as ckptr:\n",
        "  num_steps = 10\n",
        "  for step in range(num_steps):\n",
        "    saved = ckptr.save_pytree(step, pytree)\n",
        "    assert saved\n",
        "    pytree = train_step(pytree)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnDmqYR1fXfq"
      },
      "source": [
        "Calling `load` with no arguments will automatically restore the latest saved\n",
        "checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ijrvl4LfXfq"
      },
      "outputs": [],
      "source": [
        "with training.Checkpointer(root_directory) as ckptr:\n",
        "  print(ckptr.load_pytree())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKJfpLCifXfq"
      },
      "source": [
        "## Checkpointer APIs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_05nYgdfXfq"
      },
      "source": [
        "Now, let's get into a bit more detail about how to interact with the\n",
        "`Checkpointer`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tT2RQXMsfXfq"
      },
      "source": [
        "In general, we recommend using `Checkpointer` as a context manager, as shown in\n",
        "the examples below.\n",
        "\n",
        "```\n",
        "with Checkpointer(...) as ckptr:\n",
        "  ...\n",
        "```\n",
        "\n",
        "You can use it without the context manager, but make sure to call `close()`\n",
        "before the program exits to ensure the completion of any outstanding operations\n",
        "and to ensure resource cleanup.\n",
        "\n",
        "```\n",
        "ckptr = Checkpointer(...)\n",
        "...\n",
        "ckptr.close()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOv4MudgfXfq"
      },
      "source": [
        "### Saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPh3JoS6fXfq"
      },
      "source": [
        "Calling `save` in the training loop automatically calls `should_save`, which\n",
        "determines whether or not a checkpoint should be saved at the given step, based\n",
        "on the configured saving frequency. If a save is performed `save` returns\n",
        "`True`; otherwise it returns `False`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMj7jbyGfXfr"
      },
      "source": [
        "Whether or not a save should be performed can be controlled via\n",
        "`SaveDecisionPolicy`.\n",
        "\n",
        "By default, `ContinuousCheckpointingPolicy` is configured, which always saves\n",
        "*unless* a save is already ongoing.\n",
        "\n",
        "Other pre-configured policies include: - `FixedIntervalPolicy`: Saves every `n`\n",
        "steps. - `InitialSavePolicy`: Saves on the first step. -\n",
        "`PreemptionCheckpointingPolicy`: Saves on a step where a preemption signal is\n",
        "received by the JAX distributed system. This is useful for saving whenever a job\n",
        "is automatically restarted by the system. - `SpecificStepsPolicy`: Saves on the\n",
        "specific set of configured steps.\n",
        "\n",
        "The policies can be used in conjunction via `AnySavePolicy`, which performs a\n",
        "save if any of the sub-policies would perform a save at the given step.\n",
        "\n",
        "You may always implement your own policy. See `SaveDecisionPolicy` for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGzopEmVfXfr"
      },
      "outputs": [],
      "source": [
        "root_directory = epath.Path('/tmp/my-checkpoints-2')\n",
        "root_directory.rmtree(missing_ok=True)\n",
        "with training.Checkpointer(\n",
        "    root_directory,\n",
        "    save_decision_policy=training.save_decision_policies.FixedIntervalPolicy(3),\n",
        ") as ckptr:\n",
        "  for step in range(10):\n",
        "    ckptr.save_pytree(step, pytree)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLIn3QhSfXfr"
      },
      "outputs": [],
      "source": [
        "!ls {root_directory}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_Xq_xozfXfr"
      },
      "source": [
        "Now let's exercise some additional save features. These include:\n",
        "\n",
        "-   `custom_metadata`: A JSON-formatted object intended for storing any\n",
        "    user-specified properties. Custom metadata can be specified at both the root\n",
        "    directory level and the individual checkpoint level. At the root level, the\n",
        "    metadata should pertain to all checkpoints. For example, the experiment name\n",
        "    is shared by all checkpoints within the root directory, while a property\n",
        "    like `is_final` has different values for different checkpoints.\n",
        "-   `override`: Deletes and overwrites any existing checkpoint at the provided\n",
        "    step.\n",
        "-   `force`: Performs a save at the current step regardless of what would\n",
        "    ordinarily be dictated by the `SaveDecisionPolicy`.\n",
        "-   `metrics`: A JSON-formatted object storing evaluation metrics for the\n",
        "    current step. This can be useful for ordering and garbage collecting\n",
        "    checkpoints; more on that below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKyO0Ak1fXfr"
      },
      "outputs": [],
      "source": [
        "root_directory = epath.Path('/tmp/my-checkpoints-3')\n",
        "root_directory.rmtree(missing_ok=True)\n",
        "with training.Checkpointer(\n",
        "    root_directory,\n",
        "    save_decision_policy=training.save_decision_policies.FixedIntervalPolicy(3),\n",
        "    custom_metadata={'experiment_name': 'my-experiment'},\n",
        ") as ckptr:\n",
        "  num_steps = 10\n",
        "  for step in range(num_steps):\n",
        "    is_final = step == num_steps - 1\n",
        "    ckptr.save_pytree(\n",
        "        step,\n",
        "        pytree,\n",
        "        metrics={'accuracy': 0.85},\n",
        "        custom_metadata={'is_final': is_final},\n",
        "        force=is_final,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIy66LcMfXfr"
      },
      "outputs": [],
      "source": [
        "!ls {root_directory}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoluEGfGfXfr"
      },
      "source": [
        "We will learn more about how to access some of the attributes that we saved in\n",
        "the sections below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYmgp0_YfXfr"
      },
      "source": [
        "### Querying Available Checkpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FI5pmVT4fXfr"
      },
      "source": [
        "We can learn about which checkpoints are available by using `latest` and\n",
        "`checkpoints`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEEJlYM3fXfr"
      },
      "outputs": [],
      "source": [
        "ckptr = training.Checkpointer(root_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DHgY1QcfXfr"
      },
      "source": [
        "Each of these APIs returns `CheckpointMetadata` objects, which store a number of\n",
        "properties describing each checkpoint. Some metadata properties are more\n",
        "expensive to retrieve than others though. The `latest` and `checkpoints` APIs\n",
        "just store a limited set of cheaply-retrievable properties, like the `step`.\n",
        "These APIs also make use of caching as much as possible, to avoid repeated disk\n",
        "reads."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2YTgaFtfXfr"
      },
      "outputs": [],
      "source": [
        "# Returns CheckpointMetadata or None, if no checkpoints are found.\n",
        "latest = ckptr.latest\n",
        "assert latest is not None\n",
        "print(latest.step)\n",
        "print(latest)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqvLKxQQfXfr"
      },
      "source": [
        "### Inspecting Checkpoint Metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_kAy7SOfXfr"
      },
      "source": [
        "In many cases, we wish to cheaply gain information about checkpoint properties\n",
        "without loading the entire model. Using the `pytree_metadata` API, we can learn\n",
        "about the tree structure of our PyTree, as well as information about each array\n",
        "in the tree."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJKjunS0fXfr"
      },
      "source": [
        "Like loading methods, metadata methods accept either no argument, or an argument\n",
        "representing the step to retrieve metadata for.\n",
        "\n",
        "For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ty8cBpvMfXfr"
      },
      "outputs": [],
      "source": [
        "# Loads metadata from the latest checkpoint.\n",
        "ckptr.pytree_metadata()\n",
        "# Loads metadata corresponding to the first step.\n",
        "ckptr.pytree_metadata(ckptr.checkpoints[0])\n",
        "# Loads metadata from a specific integer step.\n",
        "ckptr.pytree_metadata(3)\n",
        "\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idSjSbn0fXfr"
      },
      "source": [
        "Let's examine the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73gwMap7fXfr"
      },
      "outputs": [],
      "source": [
        "ckptr.pytree_metadata()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym68LsZFfXfr"
      },
      "source": [
        "Let's dig into a few specific fields. In particular, we can access\n",
        "`custom_metadata` and `metrics` that were saved previously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugIzoaMMfXfr"
      },
      "outputs": [],
      "source": [
        "print(ckptr.pytree_metadata().metrics)\n",
        "print(ckptr.pytree_metadata().custom_metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIU7_prDfXfr"
      },
      "source": [
        "Within the metadata object, there is another field called `metadata`. This\n",
        "stores information specific to the structure of the object we saved. In this\n",
        "case, it describes the structure of the PyTree and array properties."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vvgXHj6fXfr"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "\n",
        "pprint.pprint(ckptr.pytree_metadata().metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Imb0p1cfXfr"
      },
      "source": [
        "Finally, we can also retrieve the root-level metadata. Recall that this metadata\n",
        "is intended to describe the entire sequence of checkpoints, rather than just a\n",
        "single checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pe0Z97YmfXfr"
      },
      "outputs": [],
      "source": [
        "ckptr.root_metadata()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyeMHUep1rX3"
      },
      "source": [
        "### Garbage Collection\n",
        "\n",
        "Garbage collection is important to avoid accumulating too many old checkpoints\n",
        "and running out of disk space.\n",
        "\n",
        "To control this behavior, we have an object (fairly similar to\n",
        "`SaveDecisionPolicy`) above, called `PreservationPolicy`. This class tells the\n",
        "`Checkpointer` which checkpoints should be protected from garbage collection.\n",
        "\n",
        "By default, the `PreservationPolicy` defaults to `PreserveAll` (no garbage\n",
        "collection), because we do not want users to lose any valuable data. However,\n",
        "for anything other than toy use cases,\n",
        "you should make sure to configure a more restrictive `PreservationPolicy`.\n",
        "\n",
        "Our `Checkpointer` below is implicitly configured with `PreserveAll`, so all 10\n",
        "steps should be present at first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTPQ4Kdi1wBg"
      },
      "outputs": [],
      "source": [
        "root_directory = epath.Path('/tmp/my-checkpoints-gc')\n",
        "root_directory.rmtree(missing_ok=True)\n",
        "with training.Checkpointer(root_directory) as ckptr:\n",
        "  for step in range(10):\n",
        "    ckptr.save_pytree(step, pytree)\n",
        "  print([c.step for c in ckptr.checkpoints])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dksqp_jK1x5E"
      },
      "source": [
        "If we create a new `Checkpointer` with a new `PreservationPolicy` configured,\n",
        "the same 10 checkpoints should still be present. Once we\n",
        "save a new step, any steps indicated for cleanup by the policy will be removed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovozUMzx1zd7"
      },
      "outputs": [],
      "source": [
        "with training.Checkpointer(\n",
        "    root_directory,\n",
        "    preservation_policy=training.preservation_policies.AnyPreservationPolicy([\n",
        "        training.preservation_policies.LatestN(2),\n",
        "        training.preservation_policies.EveryNSteps(4),\n",
        "    ]),\n",
        ") as ckptr:\n",
        "  print([c.step for c in ckptr.checkpoints])\n",
        "  assert ckptr.latest.step == 9\n",
        "  ckptr.save_pytree(10, pytree)\n",
        "  print([c.step for c in ckptr.checkpoints])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1myCti5I12lx"
      },
      "source": [
        "Typically, the latest `n` checkpoints are preserved (`LatestN`) along with\n",
        "checkpoints at some regular, but longer interval (`EveryNSteps` or\n",
        "`EveryNSeconds`). The latter can be useful for performing evals and maintaining\n",
        "a record of the experiment's progress."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mSBGziQFjts"
      },
      "source": [
        "### Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2FCG5h4fXfy"
      },
      "source": [
        "As we saw above with the `metadata` methods, we can load in a variety of ways."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nj6Ff2abfXfy"
      },
      "outputs": [],
      "source": [
        "# Loads from the latest checkpoint.\n",
        "ckptr.load_pytree()\n",
        "# Loads the first available checkpoint in the root directory.\n",
        "ckptr.load_pytree(ckptr.checkpoints[0])\n",
        "# Loads from a specific integer step.\n",
        "ckptr.load_pytree(4)\n",
        "\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IcfrV48fXfy"
      },
      "source": [
        "When dealing with PyTrees, particularly PyTrees with sharded `jax.Array` leaves,\n",
        "it is important for any non-toy use cases to specify an \"abstract PyTree\" that\n",
        "is used to guide restoration. Checkpoints are complicated objects. The abstract\n",
        "PyTree acts as an assertion to verify that the checkpoint has structure you\n",
        "expect and that arrays have the correct shapes.\n",
        "\n",
        "The abstract PyTree can also be used to instruct Orbax how to load the PyTree.\n",
        "The `dtype` property may be used to cast arrays, while the `sharding` property\n",
        "is used to correctly place array shards on devices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3jCWRCpfXfy"
      },
      "source": [
        "We should define an abstract tree with the same structure as the tree we\n",
        "originally saved. For the leaves, we specify different shardings than we\n",
        "originally saved with, and different dtypes as well, causing the loaded arrays\n",
        "to be cast and resharded when loading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "be5dl_gzfXfy"
      },
      "outputs": [],
      "source": [
        "sharding = jax.sharding.NamedSharding(\n",
        "    jax.sharding.Mesh(jax.devices(), ('x',)), jax.sharding.PartitionSpec('x')\n",
        ")\n",
        "abstract_pytree = {\n",
        "    'params': {\n",
        "        'layer0': jax.ShapeDtypeStruct((8, 2), np.float32, sharding=sharding),\n",
        "    },\n",
        "    'opt_state': [jax.ShapeDtypeStruct((16,), np.float32, sharding=sharding)],\n",
        "    'step': 0,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tb8SsCJmfXfy"
      },
      "outputs": [],
      "source": [
        "ckptr.load_pytree(None, abstract_pytree)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjqzGsJefXfy"
      },
      "source": [
        "More details on working with PyTrees in such a manner can be found at {doc}`Checkpointing PyTrees</guides/checkpoint/v1/checkpointing_pytrees>`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5T82A4MfXfy"
      },
      "source": [
        "### {doc}`Checkpointables and Dataset Checkpointing</guides/checkpoint/v1/checkpointables>`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3tPghvMfXfy"
      },
      "source": [
        "`Checkpointer` supports the concept of `checkpointables`. See the documentation\n",
        "on \"Working with Checkpointables\" for more information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQCwWlGwfXfy"
      },
      "source": [
        "In simplified terms, a \"checkpointable\" refers to a distinct piece of the\n",
        "overall checkpoint, which can be thought of as a bundle. The `PyTree` training\n",
        "state is one such checkpointable. The dataset iterator is another. Checkpointing\n",
        "the position of the dataset iterator can be useful to ensure training resumes\n",
        "where we were interrupted not just for the model parameters, but for the data as\n",
        "well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5pG2hENfXfy"
      },
      "source": [
        "We can see this concept in concrete terms using a Grain dataset iterator. See\n",
        "[Grain documentation](https://google-grain.readthedocs.io/en/latest/index.html)\n",
        "for more information. For our purposes, we can construct a toy dataset iterator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vx6fNmhSfXfy"
      },
      "outputs": [],
      "source": [
        "import grain\n",
        "\n",
        "dataset = iter(grain.MapDataset.range(30).batch(3).map(lambda x: x.tolist()))\n",
        "\n",
        "pytree = {\n",
        "    'params': {\n",
        "        'layer0': np.arange(16).reshape((8, 2)),\n",
        "    },\n",
        "    'opt_state': [np.arange(16)],\n",
        "}\n",
        "sharding = jax.sharding.NamedSharding(\n",
        "    jax.sharding.Mesh(jax.devices(), ('x',)), jax.sharding.PartitionSpec()\n",
        ")\n",
        "pytree = jax.tree.map(lambda x: jax.device_put(x, sharding), pytree)\n",
        "pytree['step'] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qM40f3fJfXfy"
      },
      "outputs": [],
      "source": [
        "def train_step(state, ds):\n",
        "  next(ds)  # Advances the dataset iterator\n",
        "  state['step'] += 1\n",
        "  return state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsmsWS7dfXfy"
      },
      "source": [
        "We can save ten checkpoints in sequence, including the dataset iterator,\n",
        "advancing the iterator once per step. At each step, the dataset iterator points\n",
        "to `[step*3, step*3+1, step*3+2]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1G_GALeofXfy"
      },
      "outputs": [],
      "source": [
        "root_directory = epath.Path('/tmp/my-checkpoints-4')\n",
        "root_directory.rmtree(missing_ok=True)\n",
        "num_steps = 10\n",
        "\n",
        "with training.Checkpointer(root_directory) as ckptr:\n",
        "  for step in range(num_steps):\n",
        "    ckptr.save_checkpointables(step, dict(pytree=pytree, dataset=dataset))\n",
        "    pytree = train_step(pytree, dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9_qGjK1fXfy"
      },
      "source": [
        "After loading at step `5`, `new_dataset` points to position `5` of the iterator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxPqpe50fXfy"
      },
      "outputs": [],
      "source": [
        "new_dataset = iter(\n",
        "    grain.MapDataset.range(30).batch(3).map(lambda x: x.tolist())\n",
        ")\n",
        "print(f'Initial position: {next(new_dataset)}')\n",
        "\n",
        "with training.Checkpointer(root_directory) as ckptr:\n",
        "  ckptr.load_checkpointables(5, dict(pytree=None, dataset=new_dataset))\n",
        "print(f'Loaded from checkpoint: {next(new_dataset)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BvFrMoxfXfy"
      },
      "source": [
        "It's important to note that dataset loading is stateful. You need to instantiate\n",
        "an iterator object, pass it to `load_checkpointables`, and the checkpoint state\n",
        "will be restored into the iterator state of the dataset object."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nE0XwNUVrxtN"
      },
      "source": [
        "## Training with MNIST Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlLRDeMCfeQY"
      },
      "source": [
        "In the past examples, we've used incomplete data to demonstrate Orbax functionality. Here, it's useful to demonstrate checkpointing during training loops involving real-world training data.\u003cbr\u003e We'll define our own loss function and simulate a MNIST model training loop. These functions are pulled from the [Flax docs](https://flax.readthedocs.io/en/latest/mnist_tutorial.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "up2L3GggtdSp"
      },
      "outputs": [],
      "source": [
        "import flax\n",
        "from flax import nnx\n",
        "import jax\n",
        "import optax\n",
        "from orbax.checkpoint.experimental.v1._src.training.model_helpers import DotReluDot  # Don't rely on this module!\n",
        "\n",
        "flax.config.update('flax_always_shard_variable', False)\n",
        "\n",
        "\n",
        "def loss_fn(model: DotReluDot, batch: dict) -\u003e tuple[jax.Array, jax.Array]:\n",
        "  logits = model(batch['image'])\n",
        "  loss = optax.softmax_cross_entropy_with_integer_labels(\n",
        "      logits=logits, labels=batch['label']\n",
        "  ).mean()\n",
        "  return loss, logits\n",
        "\n",
        "\n",
        "@nnx.jit\n",
        "def train_step(\n",
        "    model: DotReluDot,\n",
        "    optimizer: nnx.ModelAndOptimizer,\n",
        "    batch: dict,\n",
        "):\n",
        "  grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
        "  (loss, logits), grads = grad_fn(model, batch)\n",
        "  optimizer.update(grads)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo05nnKrvAyG"
      },
      "source": [
        "It's worth mentioning that `DotReluDot` is an example layer that subclasses `nnx.Module` and is capable of incorporating sharding information to accelerate the training process. Let's recreate a sharded environment for sake of demonstration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqcllHz0XB9q"
      },
      "outputs": [],
      "source": [
        "from jax.sharding import Mesh\n",
        "import numpy as np\n",
        "\n",
        "print(f'You have 8 “fake” JAX devices now: {jax.devices()}')\n",
        "\n",
        "mesh = Mesh(\n",
        "    devices=np.array(jax.devices()).reshape(4, 2), # Can be customized to run across multiple devices\n",
        "    axis_names=('data', 'model'),\n",
        ")\n",
        "print(mesh)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYs4ya90zOmZ"
      },
      "source": [
        "Let's go ahead and import the MNIST dataset using the `Grain` library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sEurcePzbPk"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "from orbax.checkpoint.experimental.v1._src.training.model_helpers import create_dataset\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = create_dataset('train', batch_size)\n",
        "test_ds = create_dataset('test', batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BREjoRut0Vh_"
      },
      "source": [
        "Before we start the training loop, let's initialize an abstract version of our checkpoint state, without initializing any real array values. We do this for both the model and optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "cPd7kCWQh6di"
      },
      "outputs": [],
      "source": [
        "# Create an abstract model\n",
        "abs_model = nnx.eval_shape(lambda: DotReluDot(1024, rngs=nnx.Rngs(0)))\n",
        "abs_model_state = nnx.state(abs_model)\n",
        "abs_model_state = jax.tree.map(\n",
        "    lambda a, s: jax.ShapeDtypeStruct(a.shape, a.dtype, sharding=s),\n",
        "    abs_model_state,\n",
        "    nnx.get_named_sharding(abs_model_state, mesh),\n",
        ")\n",
        "\n",
        "# Create an abstract optimizer\n",
        "abs_model_tmp = DotReluDot(1024, rngs=nnx.Rngs(0))\n",
        "abs_optimizer = nnx.eval_shape(\n",
        "    lambda: nnx.ModelAndOptimizer(abs_model_tmp, optax.adamw(0.005, 0.9))\n",
        ")\n",
        "\n",
        "# Store the abstract model and optimizer as one object\n",
        "abs_state = {\n",
        "    'params': abs_model_state,\n",
        "    'optimizer': nnx.state(abs_optimizer, nnx.optimizer.OptState),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCwZkDi5wMKR"
      },
      "source": [
        "Now, we can define our main `train()` function, throughout which we will demonstrate checkpointing. A couple notes: \u003cbr\u003e\n",
        "\n",
        "*   We use `FixedIntervalPolicy` so that our checkpoint is saved every 60 training steps.\n",
        "*   We use `nnx.state()` to convert the model object (DotReluDot) and optimizer to a checkpointable PyTree, which can then be checkpointed with `ckptr.save_pytree()`\n",
        "\n",
        "When actually loading a checkpoint, we do the following:\n",
        "* If a checkpoint exists in our current checkpoints directory, we restore the latest one.\n",
        "* If no checkpoints exist in our directory and the user provides a path to another directory, we load the checkpoint saved at that path.\n",
        "* If no checkpoints have been saved, this indicates we're entering the training loop for the first time, so we don't restore a model or optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKdtZC2nx0GF"
      },
      "outputs": [],
      "source": [
        "from typing import Callable\n",
        "from orbax.checkpoint import v1 as ocp\n",
        "\n",
        "training = ocp.training\n",
        "root_directory = epath.Path('/tmp/my-checkpoints-5')\n",
        "root_directory.rmtree(missing_ok=True)\n",
        "train_steps = 1200\n",
        "learning_rate = 0.005\n",
        "momentum = 0.9\n",
        "model_depth = 1024\n",
        "\n",
        "\n",
        "def init_or_restore(\n",
        "    ckptr: training.Checkpointer, abs_state: dict, ckpt_path: str | None\n",
        ") -\u003e tuple[DotReluDot, nnx.ModelAndOptimizer, int]:\n",
        "  model = DotReluDot(model_depth, rngs=nnx.Rngs(0))\n",
        "  optimizer = nnx.ModelAndOptimizer(model, optax.adamw(learning_rate, momentum))\n",
        "\n",
        "  if ckpt_path or ckptr.latest:\n",
        "    # If a checkpoint already exists, we restore it.\n",
        "    if ckptr.latest:\n",
        "      loaded_state = ckptr.load_pytree(abstract_pytree=abs_state)\n",
        "    else:\n",
        "      loaded_state = ocp.load_pytree(path=ckpt_path, abstract_pytree=abs_state)\n",
        "    # Update model and optimizer separately\n",
        "    nnx.update(model, loaded_state['params'])\n",
        "    nnx.update(optimizer, loaded_state['optimizer'])\n",
        "    last_step = loaded_state['optimizer']['step'].value\n",
        "  else:\n",
        "    last_step = 0\n",
        "\n",
        "  return model, optimizer, last_step\n",
        "\n",
        "\n",
        "def train(fail_fn: Callable[[int], bool] = None, ckpt_path: str = None):\n",
        "  # If step_to_restore is provided, use that to load this specific checkpoint.\n",
        "  # Otherwise, load the latest checkpoint; if none exists, start from scratch.\n",
        "  with training.Checkpointer(\n",
        "      root_directory,\n",
        "      save_decision_policy=training.save_decision_policies.FixedIntervalPolicy(\n",
        "          60\n",
        "      ),\n",
        "  ) as ckptr:\n",
        "    model, optimizer, last_step = init_or_restore(ckptr, abs_state, ckpt_path)\n",
        "\n",
        "    # Main training loop\n",
        "    with mesh:\n",
        "      for step, batch in enumerate(train_ds, start=last_step + 1):\n",
        "        if step \u003e= train_steps or (fail_fn and fail_fn(step)):\n",
        "          break\n",
        "\n",
        "        train_step(model, optimizer, batch)\n",
        "        # Save the combined state\n",
        "        state = {\n",
        "            'params': nnx.state(model),\n",
        "            'optimizer': nnx.state(optimizer, nnx.optimizer.OptState),\n",
        "        }\n",
        "        ckptr.save_pytree(step, state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJasdr9sgh77"
      },
      "source": [
        "We invoke the `train()` function. In doing so, we demonstrate a \"failure\" causing our main training loop to stop at step 600."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "iGvmxrVbf39n"
      },
      "outputs": [],
      "source": [
        "# Call train()\n",
        "def simulate_failure(step):\n",
        "    return step \u003e= 600\n",
        "\n",
        "train(fail_fn=simulate_failure)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldu4L-AyHGnm"
      },
      "source": [
        "We continue training until step 900. Our last saved checkpoint is restored automatically (step 540, because this is the last save). We now continue until step 900."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "z8rOFDWvHPO7"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "train(fail_fn = lambda x: x \u003e= 900)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tW8etqP316Og"
      },
      "source": [
        "Now, we are at step 900, let's assume we want to fine-tune the model. We restore from our latest checkpoint, which is 840, and continue training until we reach step 1200."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSxUpaCf2ejw"
      },
      "outputs": [],
      "source": [
        "train(ckpt_path=root_directory / '840')\n",
        "# Here, we can inspect specific model parameters and show the checkpoints saved in directory\n",
        "# !ls {root_directory}"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//experimental/users/zachmeyers/colab:orbax_colab",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "/piper/depot//orbax/docs/guides/checkpoint/v1/training.ipynb?workspaceId=zachmeyers:CS-training-2025-06-09_135432::citc",
          "timestamp": 1750451887559
        },
        {
          "file_id": "1FY9i7ItX7ioNzyDgM9DDxtNb1i9vfgQ-",
          "timestamp": 1746026820832
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
