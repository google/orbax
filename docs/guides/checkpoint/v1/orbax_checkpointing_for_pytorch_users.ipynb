{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiBTH8L96nCs"
      },
      "source": [
        "# Orbax Checkpointing for PyTorch Users\n",
        "\n",
        "\n",
        "This tutorial serves as an onboarding guide for developers familiar with PyTorch, aiming to smooth their transition to JAX and Orbax for checkpointing. It complements existing Orbax documentation by specifically demonstrating how to map common PyTorch practices for saving and loading models to their JAX/Orbax equivalents.\n",
        "\n",
        "\n",
        "### Core Differences\n",
        "The fundamental difference lies in how state is managed. PyTorch uses an object-oriented approach where state is typically captured in a `state_dict` (a Python dictionary).  In contrast, JAX adopts a functional paradigm where the entire training state, such as model parameters, optimizer state, and step number, is explicitly managed in a single, nested data structure called a [**PyTree**](https://docs.jax.dev/en/latest/pytrees.html). Orbax is designed to efficiently save and load these PyTrees. See [Checkpointing PyTrees](https://orbax.readthedocs.io/en/latest/guides/checkpoint/checkpointing_pytrees.html) for more detail.\n",
        "\n",
        "The following table provides a high-level, side-by-side comparison of the two approaches:\n",
        "\n",
        "| Feature | **Orbax Checkpointing** | **PyTorch** |\n",
        "| :--- | :--- | :--- |\n",
        "| **Core API** | `orbax.checkpoint.PyTreeCheckpointer` and the high-level `CheckpointManager`. | `torch.save()` and `torch.load()`.|\n",
        "| **Data Structure** | Saves a JAX **PyTree** - a nested structure containing parameters, optimizer state, and any other metadata in a single object.| Saves a standard Python `dictionary`, typically containing the model and optimizer `state_dict()`.\n",
        "| **Basic Save** | `checkpointer.save(path, args=ocp.args.StandardSave(state))` | `torch.save({'model_state_dict': model.state_dict()}, path)` |\n",
        "| **Basic Load** | `restored_state = checkpointer.restore(path)` | `model.load_state_dict(torch.load(path))`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "firDbNQ_6rC6"
      },
      "source": [
        "## 1. Setup\n",
        "\n",
        "First, we set up the necessary environment by installing the required packages and importing the modules used throughout this guide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atCJsS176ttO"
      },
      "source": [
        "### Installation\n",
        "\n",
        "Install the latest `orbax-checkpoint` for core checkpointing, along with `jax`, `flax`, and `optax` for the JAX model and optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-On_ZT7vxemB"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade orbax-checkpoint jax[cuda12] flax optax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIPQd4Ed6-lr"
      },
      "source": [
        "## 2. Checkpointing: Saving \u0026 Loading Training Progress\n",
        "\n",
        "### 2.1 PyTorch Recap: Checkpointing\n",
        "\n",
        "Let's begin with the familiar PyTorch pattern. You use `torch.save()` to store a model's `state_dict()` and other training information in a dictionary, and `torch.load()` to retrieve it. You then apply the loaded parameters to a model instance using `model.load_state_dict()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBMmBabN65Cq",
        "outputId": "79dcf36d-2b42-477b-c71d-46519432cf0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0, Loss: 0.7590\n",
            "Step 1, Loss: 0.7560\n",
            "Step 2, Loss: 0.7531\n",
            "Step 3, Loss: 0.7501\n",
            "Step 4, Loss: 0.7471\n",
            "\n",
            "Saved checkpoint to pytorch_checkpoint.pth\n",
            "Loaded checkpoint from step 5 with loss 0.7471\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import tempfile\n",
        "import shutil\n",
        "\n",
        "# Define a simple model\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(10, 5)\n",
        "        self.linear2 = nn.Linear(5, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.linear1(x))\n",
        "        return self.linear2(x)\n",
        "\n",
        "# Create model and optimizer\n",
        "model = SimpleNet()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Simulate some training\n",
        "dummy_input = torch.randn(32, 10)\n",
        "dummy_target = torch.randn(32, 1)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "for step in range(5):\n",
        "    optimizer.zero_grad()\n",
        "    output = model(dummy_input)\n",
        "    loss = loss_fn(output, dummy_target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Step {step}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Save checkpoint\n",
        "tmpdir = tempfile.mkdtemp()\n",
        "checkpoint_path = os.path.join(tmpdir, 'pytorch_checkpoint.pth')\n",
        "checkpoint = {\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'step': 5,\n",
        "    'loss': loss.item()\n",
        "}\n",
        "torch.save(checkpoint, checkpoint_path)\n",
        "print(f\"\\nSaved checkpoint to {checkpoint_path}\")\n",
        "\n",
        "# Load checkpoint\n",
        "loaded_checkpoint = torch.load(checkpoint_path)\n",
        "model.load_state_dict(loaded_checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(loaded_checkpoint['optimizer_state_dict'])\n",
        "step = loaded_checkpoint['step']\n",
        "loss = loaded_checkpoint['loss']\n",
        "\n",
        "print(f\"Loaded checkpoint from step {step} with loss {loss:.4f}\")\n",
        "shutil.rmtree(tmpdir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaJSiR7d7Nl0"
      },
      "source": [
        "### 2.2 JAX/Orbax Equivalent: Functional Checkpointing with `PyTreeCheckpointer`\n",
        "\n",
        "Now, let's look at the equivalent workflow in JAX with Orbax. We manage the entire training state (parameters, optimizer state, etc.) as a single PyTree. Orbax is designed to save and load these PyTrees efficiently.\n",
        "\n",
        "We use a [`PyTreeCheckpointer`](https://orbax.readthedocs.io/en/latest/api_reference/checkpoint.checkpointers.html#orbax.checkpoint.PyTreeCheckpointer) to save our complete training state in one go. This clear, explicit management of state is a core part of the JAX philosophy."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.linen as fnn\n",
        "import optax\n",
        "import orbax.checkpoint as ocp\n",
        "from flax.training import train_state\n",
        "import tempfile\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Define a simple model using Flax\n",
        "class SimpleNet(fnn.Module):\n",
        "    @fnn.compact\n",
        "    def __call__(self, x):\n",
        "        x = fnn.Dense(5)(x)\n",
        "        x = fnn.relu(x)\n",
        "        x = fnn.Dense(1)(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model, optimizer, and the Flax TrainState object.\n",
        "# `state` is the PyTree that holds everything we need to save.\n",
        "model = SimpleNet()\n",
        "key = jax.random.PRNGKey(42)\n",
        "dummy_input = jax.random.normal(key, (32, 10))\n",
        "params = model.init(key, dummy_input)['params']\n",
        "tx = optax.adam(0.001)\n",
        "state = train_state.TrainState.create(\n",
        "    apply_fn=model.apply,\n",
        "    params=params,\n",
        "    tx=tx\n",
        ")\n",
        "\n",
        "# Define the Training Step\n",
        "@jax.jit\n",
        "def train_step(state, batch_input, batch_target):\n",
        "    def loss_fn(params):\n",
        "        predictions = state.apply_fn({'params': params}, batch_input)\n",
        "        return jnp.mean((predictions - batch_target) ** 2)\n",
        "    grad_fn = jax.value_and_grad(loss_fn)\n",
        "    loss, grads = grad_fn(state.params)\n",
        "    state = state.apply_gradients(grads=grads)\n",
        "    return state, loss\n",
        "\n",
        "# Simulate a Few Training Steps\n",
        "dummy_target = jax.random.normal(key, (32, 1))\n",
        "for _ in range(5):\n",
        "    state, loss = train_step(state, dummy_input, dummy_target)\n",
        "    print(f\"Step {state.step}, Loss: {loss:.4f}\")\n",
        "\n",
        "# Initialize a PyTreeCheckpointer, designed for saving single PyTrees.\n",
        "checkpointer = ocp.PyTreeCheckpointer()\n",
        "checkpoint_dir = tempfile.mkdtemp()\n",
        "checkpoint_path = os.path.join(checkpoint_dir, 'my_checkpoint')\n",
        "\n",
        "# Use `ocp.args.PyTreeSave` to wrap the state.\n",
        "save_args = ocp.args.PyTreeSave(item=state)\n",
        "checkpointer.save(checkpoint_path, args=save_args, force=True)\n",
        "print(f\"\\nSaved checkpoint to {checkpoint_dir}\")\n",
        "\n",
        "# Restore Checkpoint by providing the original `state` as a template.\n",
        "restored_state = checkpointer.restore(checkpoint_path, item=state)\n",
        "\n",
        "print(f\"Loaded checkpoint at step {restored_state.step}\")\n",
        "\n",
        "# Verify that the parameters of the original state and the restored state are identical.\n",
        "are_params_equal = jax.tree_util.tree_all(\n",
        "    jax.tree_util.tree_map(lambda x, y: jnp.allclose(x, y), state.params, restored_state.params)\n",
        ")\n",
        "print(\"Parameters match:\", are_params_equal)\n",
        "\n",
        "# Clean up the temporary directory.\n",
        "shutil.rmtree(checkpoint_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMf1ZUqf7-54",
        "outputId": "8096165c-ea8a-49f1-f661-c9063a7e5bd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:[process=0][thread=MainThread][operation_id=1] _SignalingThread.join() waiting for signals ([]) blocking the main thread will slow down blocking save times. This is likely due to main thread calling result() on a CommitFuture.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1, Loss: 1.7981\n",
            "Step 2, Loss: 1.7822\n",
            "Step 3, Loss: 1.7664\n",
            "Step 4, Loss: 1.7507\n",
            "Step 5, Loss: 1.7352\n",
            "\n",
            "Saved checkpoint to /tmp/tmpphbau1yv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/orbax/checkpoint/_src/serialization/type_handlers.py:1269: UserWarning: Sharding info not provided when restoring. Populating sharding info from sharding file. Please note restoration time will be slightly increased due to reading from file. Note also that this option is unsafe when restoring on a different topology than the checkpoint was saved with.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded checkpoint at step 5\n",
            "Parameters match: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cktZFfPZ4tGx"
      },
      "source": [
        "### 2.3 Advanced Checkpoint Management with `CheckpointManager`\n",
        "For robust training loops, Orbax provides the [`CheckpointManager`](https://orbax.readthedocs.io/en/latest/api_reference/checkpoint.checkpoint_manager.html#id1). This high-level utility automates the checkpointing process, handling saving and cleaning up old checkpoints based on rules you define, such as keeping only the N most recent checkpoints. This ensures a clean directory and simplifies training state management."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd5742c2",
        "outputId": "7f4b74d9-2f12-4a11-8ffe-e6c5d0a545ee"
      },
      "source": [
        "from orbax.checkpoint import CheckpointManager, CheckpointManagerOptions\n",
        "\n",
        "# Define state initialization\n",
        "def init_state():\n",
        "    key = jax.random.PRNGKey(42)\n",
        "    dummy_input = jax.random.normal(key, (32, 10))\n",
        "    model = SimpleNet()\n",
        "    params = model.init(key, dummy_input)['params']\n",
        "    tx = optax.adam(0.001)\n",
        "    return train_state.TrainState.create(\n",
        "        apply_fn=model.apply,\n",
        "        params=params,\n",
        "        tx=tx\n",
        "    )\n",
        "\n",
        "# Define the training step function once, outside the loop.\n",
        "@jax.jit\n",
        "def train_step(state, batch_input, batch_target):\n",
        "    def loss_fn(params):\n",
        "        predictions = state.apply_fn({'params': params}, batch_input)\n",
        "        return jnp.mean((predictions - batch_target) ** 2)\n",
        "    grad_fn = jax.value_and_grad(loss_fn)\n",
        "    loss, grads = grad_fn(state.params)\n",
        "    state = state.apply_gradients(grads=grads)\n",
        "    return state, loss\n",
        "\n",
        "# --- Main Logic ---\n",
        "state = init_state()\n",
        "key = jax.random.PRNGKey(42)\n",
        "dummy_input = jax.random.normal(key, (32, 10))\n",
        "dummy_target = jax.random.normal(key, (32, 1))\n",
        "managed_checkpoint_dir = tempfile.mkdtemp()\n",
        "options = CheckpointManagerOptions(max_to_keep=3, create=True)\n",
        "checkpoint_manager = CheckpointManager(managed_checkpoint_dir, options=options)\n",
        "\n",
        "print(\"Training with automatic checkpoint management:\")\n",
        "for step in range(1, 21):\n",
        "    state, loss = train_step(state, dummy_input, dummy_target)\n",
        "    print(f\"Step {state.step}: Loss: {loss:.4f}\")\n",
        "\n",
        "    if step % 6 == 0:\n",
        "        checkpoint_manager.save(step, args=ocp.args.StandardSave(state))\n",
        "        print(f\"  -\u003e Saved checkpoint for step {step}\")\n",
        "\n",
        "\n",
        "print(f\"\\nAvailable checkpoints: {checkpoint_manager.all_steps()}\")\n",
        "latest_step = checkpoint_manager.latest_step()\n",
        "print(f\"Latest checkpoint step: {latest_step}\")\n",
        "\n",
        "# Restore from step 18\n",
        "abstract_state = jax.eval_shape(init_state)\n",
        "restored_state = checkpoint_manager.restore(\n",
        "    latest_step,\n",
        "    args=ocp.args.StandardRestore(abstract_state)\n",
        ")\n",
        "\n",
        "print(f\"\\nRestored checkpoint from step {restored_state.step}\")\n",
        "\n",
        "shutil.rmtree(managed_checkpoint_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with automatic checkpoint management:\n",
            "Step 1: Loss: 1.7981\n",
            "Step 2: Loss: 1.7822\n",
            "Step 3: Loss: 1.7664\n",
            "Step 4: Loss: 1.7507\n",
            "Step 5: Loss: 1.7352\n",
            "Step 6: Loss: 1.7198\n",
            "  -\u003e Saved checkpoint for step 6\n",
            "Step 7: Loss: 1.7046\n",
            "Step 8: Loss: 1.6895\n",
            "Step 9: Loss: 1.6746\n",
            "Step 10: Loss: 1.6598\n",
            "Step 11: Loss: 1.6452\n",
            "Step 12: Loss: 1.6308\n",
            "  -\u003e Saved checkpoint for step 12\n",
            "Step 13: Loss: 1.6165\n",
            "Step 14: Loss: 1.6024\n",
            "Step 15: Loss: 1.5885\n",
            "Step 16: Loss: 1.5749\n",
            "Step 17: Loss: 1.5614\n",
            "Step 18: Loss: 1.5480\n",
            "  -\u003e Saved checkpoint for step 18\n",
            "Step 19: Loss: 1.5348\n",
            "Step 20: Loss: 1.5218\n",
            "\n",
            "Available checkpoints: [6, 12, 18]\n",
            "Latest checkpoint step: 18\n",
            "\n",
            "Restored checkpoint from step 18\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
