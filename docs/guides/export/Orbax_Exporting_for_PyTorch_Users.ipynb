{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Orbax Export for PyTorch Users\n",
        "This tutorial is a guide for developers familiar with PyTorch, aiming to smooth their transition to JAX and Orbax for model exporting. It complements the existing Orbax documentation by demonstrating how to map the common PyTorch practice of exporting models for inference to its JAX/Orbax equivalent.\n",
        "\n",
        "## Core Differences\n",
        "PyTorch's `torch.export` tool creates a self-contained ExportedProgram that includes the model's graph and weights. This program can be saved for deployment. JAX, on the other hand, is a functional framework that needs both the model's parameters and its forward pass function. The parameters are stored in a [PyTree](https://www.google.com/url?q=https%3A%2F%2Fdocs.jax.dev%2Fen%2Flatest%2Fpytrees.html). Orbax's export utilities package these two components into the widely-used TensorFlow SavedModel format, which allows JAX models to be served in production environments like TensorFlow Serving.\n",
        "\n",
        "The following table provides a high-level, side-by-side comparison of the two approaches:\n",
        "\n",
        "| Feature | **Orbax Export** | **PyTorch** |\n",
        "| :--- | :--- | :--- |\n",
        "| **Core API** | `orbax.export.JaxModule and orbax.export.ExportManager`. | `torch.export.export(), torch.export.save(), and torch.export.load()`.|\n",
        "| **Data Structure** | Packages a JAX PyTree of parameters along with a Python function (e.g., `model.apply`).| Creates an `ExportedProgram` object containing the model's graph, state dictionary, and buffers.\n",
        "| **Output Format** | TensorFlow SavedModel | A self-contained `.pt2` file format |\n",
        "| **Basic Workflow** | Wrap parameters and apply function in a `JaxModule`, then use `ExportManager` to save as a `SavedModel.` | Pass the model and inputs to `torch.export.export()` to generate an `ExportedProgram`, then save it."
      ],
      "metadata": {
        "id": "e4TLF6e1LJq1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup\n",
        "\n",
        "### Installation\n",
        "\n",
        "Start by installing the required packages. This includes `orbax.export` for the main export features, `jax` and `flax` for model building, and tensorflow for the target export format. `torch` is also included for the comparative example."
      ],
      "metadata": {
        "id": "STrCB0VpLpDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q orbax-export jax[cuda12] flax tensorflow torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqoOoLmXOURe",
        "outputId": "cfbaef3f-cb62-408a-bb98-1a12a01c069d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/180.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.5/180.5 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/55.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Model Export: Preparing Models for Inference\n",
        "This section covers how to prepare a trained model for deployment, starting with the standard PyTorch approach and then showing the equivalent method using JAX and Orbax.\n",
        "\n",
        "### 2.1 PyTorch Recap: Exporting with `torch.export`\n",
        "The modern approach to exporting PyTorch models is to use the torch.export library. This tool traces the model's execution using sample inputs to produce an ExportedProgram. This object is a portable and standardized representation of the model, encapsulating both the computation graph and the learned weights (`state_dict`). The program can be saved as a `.pt2` file, which can then be loaded in other environments for efficient inference."
      ],
      "metadata": {
        "id": "KGdNvlmtOlaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "# Define a simple PyTorch model\n",
        "class PyTorchSimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(10, 5)\n",
        "        self.linear2 = nn.Linear(5, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.linear1(x))\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "# Create model instance and dummy input for tracing\n",
        "pytorch_model = PyTorchSimpleNet()\n",
        "pytorch_model.eval()  # Set to evaluation mode\n",
        "dummy_input = torch.randn(1, 10) # '1' for batch size\n",
        "\n",
        "# Export the model to an ExportedProgram\n",
        "exported_program = torch.export.export(pytorch_model, (dummy_input,))\n",
        "print(\"Model successfully exported to an ExportedProgram.\")\n",
        "\n",
        "# Save the exported program to a file\n",
        "tmpdir = tempfile.mkdtemp()\n",
        "EXPORT_PATH = os.path.join(tmpdir, 'exported_model.pt2')\n",
        "torch.export.save(exported_program, EXPORT_PATH)\n",
        "print(f\"ExportedProgram saved to {EXPORT_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmZlE8XnOdEq",
        "outputId": "f05d13b2-cfac-45e1-ff9b-997fcaeefef8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model successfully exported to an ExportedProgram.\n",
            "ExportedProgram saved to exported_model.pt2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Loading and Verifying the Exported Model\n",
        "After saving the model, it's crucial to verify its integrity. The saved `.pt2` file can be loaded back into a new ExportedProgram object using `torch.export.load()`. You can then run inference on both the original and loaded models with the same input to ensure their outputs are identical."
      ],
      "metadata": {
        "id": "lY3gW8xs4vPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the program and verify\n",
        "loaded_program = torch.export.load(EXPORT_PATH)\n",
        "print(\"ExportedProgram loaded successfully.\")\n",
        "\n",
        "# Run inference with both original and loaded models to ensure they match\n",
        "with torch.no_grad():\n",
        "    original_output = pytorch_model(dummy_input)\n",
        "    loaded_output = loaded_program.module()(dummy_input)\n",
        "\n",
        "diff = torch.max(torch.abs(original_output - loaded_output)).item()\n",
        "print(f\"Output difference between original and loaded models: {diff:.6f}\")\n",
        "\n",
        "# Clean up\n",
        "os.remove(EXPORT_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBQOlWC1gTx6",
        "outputId": "ca3109cb-6c8b-4843-d73e-d84821fdd7fd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ExportedProgram loaded successfully.\n",
            "Output difference between original and loaded models: 0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Exporting JAX/Flax Models with Orbax\n",
        "\n",
        "### 3.1 JAX/Orbax Equivalent: Functional Export with `JaxModule`\n",
        "\n",
        "Model export in the JAX ecosystem focuses on converting models to the standard TensorFlow **SavedModel** format, which is ideal for production serving. The process involves two main components from `orbax.export`:\n",
        "\n",
        "1. [`JaxModule`](https://orbax.readthedocs.io/en/latest/api_reference/export.jax_module.html): This class acts as a container, bundling your model's parameters (a PyTree) with its forward pass function (`apply_fn`). This treats the entire model as a single, exportable unit.\n",
        "\n",
        "2. [`ExportManager`](https://orbax.readthedocs.io/en/latest/api_reference/export.export_manager.html): This utility orchestrates the conversion of a `JaxModule` into a TensorFlow SavedModel, saving the result to a specified directory.\n",
        "\n"
      ],
      "metadata": {
        "id": "W9yac8K1Q1g6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Imports"
      ],
      "metadata": {
        "id": "W11SIJVWokRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from flax import linen as nn\n",
        "from typing import Sequence\n",
        "import tensorflow as tf\n",
        "from orbax.export import JaxModule\n",
        "from orbax.export import ServingConfig, ExportManager\n",
        "import shutil\n"
      ],
      "metadata": {
        "id": "jBLkhKmrk6Gr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 JAX Model Definition and Preparation\n",
        "\n",
        "To begin, define a simple MLP using Flax. Unlike stateful PyTorch models, a Flax model is defined by its architecture and needs a separate **PyTree** of parameters (`initial_params`) to be initialized."
      ],
      "metadata": {
        "id": "ssS_7572oiUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the JAX/Flax Model\n",
        "class SimpleMLP(nn.Module):\n",
        "    features: Sequence[int]\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        # Flatten the input image\n",
        "        x = x.reshape((x.shape[0], -1))\n",
        "        for i, dim in enumerate(self.features):\n",
        "            x = nn.Dense(features=dim, name=f'dense_{i}')(x)\n",
        "            if i \u003c len(self.features) - 1:\n",
        "                x = nn.relu(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model and its parameters\n",
        "INPUT_DIM = (28, 28, 1)\n",
        "OUTPUT_FEATURES = 10\n",
        "model = SimpleMLP(features=[128, 64, OUTPUT_FEATURES])\n",
        "key = jax.random.PRNGKey(42)\n",
        "\n",
        "# Create dummy input to initialize the model's parameters\n",
        "dummy_input = jnp.ones((1, *INPUT_DIM), dtype=jnp.float32)\n",
        "initial_params = model.init(key, dummy_input)['params']\n",
        "\n",
        "print(\"JAX Model and Parameters Initialized\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmFVt29LQprY",
        "outputId": "b932628a-0f4a-422f-840f-79ffb8a00a19"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JAX Model and Parameters Initialized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Packaging the Model with JaxModule\n",
        "Next, define the forward pass function and package it with the `initial_params` into a `JaxModule`. The `input_polymorphic_shape` is specified to allow the exported model to handle variable batch sizes, denoted by `'b'`."
      ],
      "metadata": {
        "id": "miVtswxCRzqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the apply function that will be exported\n",
        "def apply_fn(params, x):\n",
        "    return model.apply({'params': params}, x)\n",
        "\n",
        "# Wrap the parameters and function in a JaxModule\n",
        "jax_module = JaxModule(\n",
        "    params=initial_params,\n",
        "    apply_fn=apply_fn,\n",
        "    input_polymorphic_shape=\"b, 28, 28, 1\"\n",
        ")\n",
        "\n",
        "print(\"JaxModule Created\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmepN2YWRmld",
        "outputId": "99acbb69-7d31-4818-819e-90f90e137194"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JaxModule Created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5 Exporting to SavedModel with `ExportManager`\n",
        "Next, with the `JaxModule` prepared, configure its serving signature using [`ServingConfig`](https://orbax.readthedocs.io/en/latest/api_reference/export.serving_config.html). This step defines the expected shape and name of the input tensor for the TensorFlow graph. Finally, the `ExportManager` takes the module and its configuration to save into the **SavedModel**.\n"
      ],
      "metadata": {
        "id": "AQnSYw5wSIGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SAVE_PATH = os.path.join(tmpdir, \"orbax_exported_savedmodel\")\n",
        "\n",
        "# Define the serving signature\n",
        "serving_config = ServingConfig(\n",
        "    signature_key=\"serving_default\",\n",
        "    input_signature=[\n",
        "        tf.TensorSpec(shape=(None, *INPUT_DIM), dtype=tf.float32, name='input_image')\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Use ExportManager to save the JaxModule as a TensorFlow SavedModel\n",
        "export_manager = ExportManager(\n",
        "    module=jax_module,\n",
        "    serving_configs=[serving_config]\n",
        ")\n",
        "\n",
        "print(f\"Exporting model to: {SAVE_PATH}...\")\n",
        "export_manager.save(model_path=SAVE_PATH)\n",
        "print(\"Export to SavedModel Complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GI_63IOMSCR-",
        "outputId": "817ed6bd-e0d4-4e3e-86fe-3b696d081873"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exporting model to: ./orbax_exported_savedmodel...\n",
            "Export to SavedModel Complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.6 Verification in TensorFlow\n",
        "To verify a successful export, the **SavedModel** is loaded using TensorFlow's standard library. Inference is then run on a sample batch of data, and the output is compared to the original JAX model's output to ensure numerical consistency."
      ],
      "metadata": {
        "id": "BTXdnqgxScFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load the SavedModel from the specified path\n",
        "print(f\"Loading SavedModel from: {SAVE_PATH}...\")\n",
        "loaded_model = tf.saved_model.load(SAVE_PATH)\n",
        "loaded_signature = loaded_model.signatures['serving_default']\n",
        "print(\"SavedModel loaded successfully.\")\n",
        "\n",
        "# Create a test input with a dynamic batch size\n",
        "test_batch_size = 5\n",
        "test_input_np = np.random.rand(test_batch_size, *INPUT_DIM).astype(np.float32)\n",
        "tf_input = tf.constant(test_input_np)\n",
        "\n",
        "# Run inference using the loaded TensorFlow model\n",
        "tf_output = loaded_signature(input_image=tf_input)\n",
        "tf_output_array = tf_output['output_0'].numpy()\n",
        "\n",
        "print(f\"Inference successful. Output shape: {tf_output_array.shape}\")\n",
        "\n",
        "# Compare outputs between the original JAX model and the loaded TF model\n",
        "jax_output = apply_fn(initial_params, jnp.asarray(test_input_np))\n",
        "match = np.allclose(jax_output, tf_output_array, atol=1e-5)\n",
        "print(f\"Numerical check (JAX vs. SavedModel): {'MATCH' if match else 'MISMATCH'}\")\n",
        "\n",
        "# Clean up the exported directory\n",
        "shutil.rmtree(SAVE_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HONrMT3BSXLb",
        "outputId": "09fbe97b-5722-4c7c-fbca-8b30b962f14c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading SavedModel from: ./orbax_exported_savedmodel...\n",
            "SavedModel loaded successfully.\n",
            "Inference successful. Output shape: (5, 10)\n",
            "Numerical check (JAX vs. SavedModel): MATCH\n"
          ]
        }
      ]
    }
  ]
}
