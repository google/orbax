# Copyright 2025 The Orbax Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Benchmarks for orbax.checkpoint.PyTreeCheckpointHandler."""

from __future__ import annotations

import dataclasses
import pprint
import time
from typing import Any

from absl import logging
import jax
from jax.experimental import multihost_utils
import numpy as np
import orbax.checkpoint as ocp_v0  # pylint: disable=unused-import
from orbax.checkpoint import v1 as ocp
from orbax.checkpoint._src.testing.benchmarks.core import core as benchmarks_core
from orbax.checkpoint._src.testing.benchmarks.core import metric as metric_lib
import requests


SERVICE_URL = "http://service-dns/"


def _metrics_to_measure(options: LustreBenchmarkOptions) -> list[str]:
  """Returns the list of metrics to measure."""
  del options
  metrics = ["time", "rss", "io"]
  return metrics


# ==============================================================================
# 1. Define the Options Dataclass for this specific benchmark
# ==============================================================================
@dataclasses.dataclass(frozen=True)
class LustreBenchmarkOptions(benchmarks_core.BenchmarkOptions):
  """Configuration options for benchmarks targeting PyTreeCheckpointHandler.

  Each attribute can be a single value or a list of values to create
  a parameter sweep.

  Attributes:
    use_ocdbt: Whether to use OCDBT for checkpointing.
  """

  use_ocdbt: bool = True

  def is_valid(self):
    return True


class StorageServiceClient:
  """Docstring."""

  def __init__(self, service_url: str | None = None):
    self._service_url = service_url or SERVICE_URL

  def resolve(self, execution_id: int, step: int) -> str:
    """Resolves an asset path from the service."""
    start = time.time()
    logging.info("Resolving ID-step: %s-%s.", execution_id, step)
    payload = {"execution_id": execution_id, "step": step}
    response = requests.post(f"{self._service_url}/resolve", json=payload)
    logging.info("Response: %s", response.json())
    response.raise_for_status()
    result = response.json()["path"]
    end = time.time()
    logging.info("Resolved %s in %s seconds.", result, end - start)
    return result

  def finalize(self, execution_id: int, step: int) -> None:
    """Finalizes an asset in the service."""
    start = time.time()
    payload = {"execution_id": execution_id, "step": step}
    response = requests.post(f"{self._service_url}/finalize", json=payload)
    response.raise_for_status()
    logging.info(response)
    # assert response.json()["status"] == "ok"
    end = time.time()
    logging.info(
        "Finalized %s %s in %s seconds.", execution_id, step, end - start
    )


def _get_xid() -> int:
  """Returns the XID for this run."""
  xid = multihost_utils.broadcast_one_to_all(
      np.asarray(int(time.time()))
  ).item()
  logging.info("XID: %s", xid)
  return xid


# ==============================================================================
# 2. Implement the Benchmark Generator
# ==============================================================================
@benchmarks_core.benchmark_options(LustreBenchmarkOptions)
class LustreBenchmark(benchmarks_core.BenchmarksGenerator):
  """Docstring."""

  def __init__(self, *args, **kwargs):
    super().__init__(*args, **kwargs)
    self._client = StorageServiceClient()
    self._xid = _get_xid()

  def _clear_pytree(self, pytree: Any) -> Any:
    """Clears the pytree to free up memory."""
    return jax.tree.map(
        lambda x: x.delete() if isinstance(x, jax.Array) else None, pytree
    )

  def test_fn(
      self, context: benchmarks_core.TestContext
  ) -> benchmarks_core.TestResult:
    """The core test logic for a single save/restore cycle.

    This function is called for each combination of options generated by the
    framework. It uses the `context.options` to configure the handler
    dynamically for each run.

    Args:
      context: The test context containing the pytree, path, and options.

    Returns:
      The test result containing the metrics.
    """
    logging.info(
        "JAX info: %s processes, %s devices, %s process index",
        jax.process_count(),
        jax.device_count(),
        jax.process_index(),
    )
    metrics = metric_lib.Metrics()
    pytree = context.pytree
    options = context.options
    assert isinstance(options, LustreBenchmarkOptions)

    logging.info("Benchmark options: %s", pprint.pformat(options))

    metrics_to_measure = _metrics_to_measure(options)

    step = context.repeat_index or 0

    with metrics.measure("resolve_cache", metrics_to_measure):
      resolved_path = self._client.resolve(self._xid, step)
    with metrics.measure("save_cache", metrics_to_measure):
      ocp.save_pytree(resolved_path, pytree)
    with metrics.measure("finalize_cache", metrics_to_measure):
      self._client.finalize(self._xid, step)
    with metrics.measure("restore_cache", metrics_to_measure):
      restored_pytree = ocp.load_pytree(resolved_path, pytree)
    self._clear_pytree(restored_pytree)

    with metrics.measure("save", metrics_to_measure):
      ocp.save_pytree(context.path / str(step), pytree)
    with metrics.measure("restore", metrics_to_measure):
      restored_pytree = ocp.load_pytree(context.path / str(step), pytree)
    self._clear_pytree(restored_pytree)

    return benchmarks_core.TestResult(metrics=metrics)
